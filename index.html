<!DOCTYPE html>
<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>

a {
            text-decoration: none;
            color: blue;         /* Set hyperlink text color to blue */
  }

<!--
 /* List Definitions */
 ol
	{margin-bottom:0in;}
 ul
	{margin-bottom:0in;}
-->
</style>

</head>

<body>

    <p><img width=624 height=45 src="images/image1.gif"></p>

    <p><b>TLDR: We are hiring!</b> Specifically, we are hiring PhDs graduating in 2025, professionals with  industry experience, and managers. We are NOT hiring new graduates with a bachelor's or master's degree. Our summer internship positions have been filled. Candidates please email us at "<i>codesign AT meta DOT com</i>". Curious about Meta's fascinating hyperscale infrastructure? Check out our brief <a href="https://dl.acm.org/doi/pdf/10.1145/3701296">article</a> for an inside look.</p>

<p> The AI and Systems Co-Design team at Meta (formerly known as Facebook), led by <a href="https://tangchq74.github.io/"><a href="https://sites.google.com/site/tangchq">Chunqiang Tang</a> (a.k.a. CQ Tang), consists of over 100 employees, mostly PhDs, including many world-class research scientists and engineers. As reflected in our team name "co-design", we conduct interdisciplinary research and development across AI, hardware, and software, with a focus on performance, efficiency, and scalability.

<ul style='margin-top:-0.1in'>
    <li>We own the company's overall strategy for exploring innovative hardware technologies for CPUs, GPUs, memory, storage, and Meta's custom AI chips, and we productionize them in Meta's hyperscale fleet of O(1,000,000) servers and O(100,000) GPUs, powering all Meta products such as Facebook, Instagram, and <a href="http://meta.ai/">Meta AI</a>.</li>

    <li>We apply novel software optimizations across the whole stack---from ML models and applications to the Linux kernel---to achieve optimal performance on the hardware.</li>

    <li>We develop innovative AI technologies for large language models (Llama), ranking systems, and more.</li>
</ul>

<p>Overall, our work largely corresponds to the research communities of systems in general and especially systems for ML (MLSys, SOSP, OSDI, SIGCOMM, NSDI), hardware architecture (ISCA, ASPLOS), ML (NeurIPS, ICML, ICLR) and supercomputing (SC, ICS). Here are selected publications that showcase our work in diverse areas:</p>

<ul style='margin-top:-0.1in'>

<li>Systems for ML
    <ul>
     <li>[OSDI'25] WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training</li>
        <li>[ISCA'25] Scaling Llama 3 Training with Efficient Parallelism Strategies</li>

    <li>[OSDI'24] <a href="https://www.usenix.org/system/files/osdi24-choudhury.pdf"> MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale</a></li>


    <li>[ISCA'22] <a href="https://arxiv.org/pdf/2104.05158">Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models</a></li>

    <li>[VLDB'23] <a href="https://arxiv.org/pdf/2304.11277">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</a></li>

        <li><a href="https://ai.meta.com/research/publications/the-llama-3-herd-of-models/">The Llama 3 Herd of Models</a>. Our contributions include much of the work described in the paper's Section 3.3 "Infrastructure, Scaling, and Efficiency", Section 6 "Inference", and Section 7.3 "Model Scaling".</li>

    </ul>
</li>

<li>AI chip
    <ul>
        <li>[ISCA'25] Meta's Second Generation AI Chip: Model-Chip Co-Design and Productionization Experiences</li>
        <li>[ISCA'23] <a href="http://firoozshahian.com/publications/3579371.3589348.pdf">MTIA: First Generation Silicon Targeting Meta's Recommendation Systems</a>
    </ul>
</li>

<li>ML models and kernels
    <ul>
        <li>[ICML'24] <a href="https://arxiv.org/pdf/2403.02545">Wukong: Towards a Scaling Law for Large-Scale Recommendation</a></li>

        <li><a href="https://arxiv.org/pdf/1906.00091">Deep Learning Recommendation Model for Personalization and Recommendation Systems</a></li>

        <li><a href="https://arxiv.org/pdf/2101.05615">FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference</a></li>
    </ul>
</li>

<li>ML numerics, pruning, distillation, and optimizer
    <ul>
        <li>[ISCA'23] <a href="https://arxiv.org/abs/2302.08007">Shared Microexponents: A Little Shifting Goes a Long Way</a></li>

        <li><a href="https://arxiv.org/pdf/2310.10537">Microscaling Data Formats for Deep Learning</a></li>

        <li><a href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Pruning and Distillation to Enable Llama 3.2 1B and 3B Models Suitable for Mobile Devices</a></li>

        <li><a href="https://www.linkedin.com/posts/hjmshi_announcing-the-results-of-the-inaugural-algoperf-activity-7224879495734870017-rVHn/"> PyTorch Distributed Shampoo Winning the MLCommon Training
Algorithms Competition</a></li>
    </ul>
</li>

<li>HPC and collective communications library (MPI, NCCL, RCCL)
    <ul>
        <li>[SC'24] <a href="https://arxiv.org/pdf/2407.04272">Accelerating
Communication in Deep Learning Recommendation Model Training with Dual-Level
Adaptive Lossy Compression</a></li>

        <li>[DLP-KDD'20] <a href="https://research.fb.com/wp-content/uploads/2020/08/Training-Deep-Learning-Recommendation-Model-with-Quantized-Collective-Communications-v2.pdf"> Training Deep Learning Recommendation Model with Quantized Collective Communications</a></li>
    </ul>
</li>


<li>Performance benchmarking and projection for both AI and non-AI workloads
    <ul>
        <li>[ISCA'25] DCPerf: An Open-Source, Battle-Tested Performance Benchmark Suite for Datacenter Workloads</li>

        <li><a href="https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/">DLRM: An advanced, open source deep learning recommendation model</a></li>
    </ul>
</li>

<li>Hardware and software co-design
    <ul>
        <li>[<b>ISCA'23 Best Paper</b>] <a href="https://dl.acm.org/doi/pdf/10.1145/3579371.3589079"> Contiguitas: The Pursuit of Physical Memory Contiguity in Datacenters</a></li>

        <li>[ISCA'23] <a href="https://arxiv.org/pdf/2206.02878">TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory</a></li> 

        <li>[ISCA'19] <a href="https://research.fb.com/wp-content/uploads/2019/05/SoftSKU-Optimizing-Server-Architectures-for-Microservice-Diversity-@Scale.pdf">SoftSKU: Optimizing Server Architectures for Microservice
Diversity @Scale</a></li>
    </ul>
</li>
</ul>

<p>Like research labs, our team consists primarily of PhDs, and we strongly encourage and excel in research publications. However, we differ from traditional research labs in several key ways:

<ul style='margin-top:-0.1in'>
    <li><i>Direct ownership</i>: Like traditional research labs, we build strong partnerships with numerous teams across diverse areas for broad influence. However, what sets us apart is our direct ownership of the hardware strategy for Meta's hyperscale fleet. This enables us to lead in many areas while fostering seamless partnerships in others.</li>

    <li><i>Production systems</i>: Our primary goal is to develop forward-looking innovations in AI, hardware, and software, and directly implement them in production systems that serve billions of people. The billions of users of Meta products and Meta's hyperscale fleet of O(1,000,000) servers and O(100,000) GPUs are, in effect, our lab. In contrast, traditional research labs often rely on technology transfer for a less direct impact.</li>

    <li><i>Impact</i>: Our impact is widely acknowledged within the company and throughout the industry. We drive Meta's hardware strategy to save billions of dollars, and directly develop innovative technologies in Meta's flagship products like Llama and Ads ranking models.</li>
</ul>

<!--
<div id="members">
<p>To gain a better understanding of who we are, feel free to explore some profiles of our team members:
<a href="https://tangchq74.github.io/">Chunqiang (CQ) Tang</a>,
<a href="https://scholar.google.com/citations?user=8xbO51UAAAAJ&amp;hl=en&amp;oi=ao">Pavan Balaji</a>,
<a href="https://scholar.google.com/citations?user=HqGW4HIAAAAJ&amp;hl=en&amp;oi=ao">Amar Phanishayee</a>,
<a href="https://scholar.google.com/citations?user=p5h2zh8AAAAJ&amp;hl=en&amp;oi=ao">Maxim Naumov</a>,
<a href="https://sites.google.com/site/jongsoopark/home">Jongsoo Park</a>,
<a href="https://scholar.google.com/citations?user=eOyDnQYAAAAJ&amp;hl=en"> Changkyu Kim</a>,
<a href="https://scholar.google.com/citations?user=yxUD8q4AAAAJ&amp;hl=en&amp;oi=ao">Doe Hyun Yoon</a>,
<a href="https://scholar.google.com/citations?user=yUXXPwYAAAAJ&amp;hl=en">Satish Nadathur</a>,
<a href="https://scholar.google.com/citations?user=8rbsmO0AAAAJ&amp;hl=en&amp;oi=ao">Abhishek Dhanotia</a>,
<a href="https://scholar.google.com/citations?user=KpnlsFwAAAAJ&amp;hl=en&amp;oi=ao">Joel Coburn</a>,
<a href="https://scholar.google.com/citations?user=6PyfjT4AAAAJ&amp;hl=en&amp;oi=ao">Ehsan K. Ardestani</a>,
<a href="https://scholar.google.com/citations?user=Ce2uZUYAAAAJ&amp;hl=en&amp;oi=ao">Bangsheng Tang</a>,
<a href="https://scholar.google.com/citations?user=mD2TUGQAAAAJ&amp;hl=en&amp;oi=ao">Mustafa Ozdal</a>,<a
<a href="https://jianyuhuang.com/">Jianyu Huang</a>,
<a href="https://dblp.org/pid/92/5162.html">Wenyin Fu</a>,
<a href="https://scholar.google.com/citations?user=nFONGREAAAAJ&amp;hl=en">Jayneel Gandhi</a>,
<a href="https://scholar.google.com/citations?user=elK4Gu8AAAAJ&amp;hl=en">Geet Sethi</a>.</li></p>
</div>
-->

<h3>Open Source Projects</h3>
<ul style='margin-top:-0.1in'>
    <li><a href="https://engineering.fb.com/2024/08/05/data-center-engineering/dcperf-open-source-benchmark-suite-for-hyperscale-compute-applications/">DCPerf</a>: An open source benchmark suite for hyperscale compute applications</li>

    <li><a href="https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/"> DLRM</a>: An advanced, open source deep learning recommendation model</li>

    <li><a href="https://pytorch.org/FBGEMM/">FBGEMM</a>: ML kernels</li>

    <li><a href="https://github.com/facebookresearch/optimizers/blob/main/distributed_shampoo/README.md"> Pytorch distributed Shampoo optimizer</a>. This work <a href="https://www.linkedin.com/posts/hjmshi_announcing-the-results-of-the-inaugural-algoperf-activity-7224879495734870017-rVHn/">won</a> the competition of the external tuning track of the inaugural AlgoPerf training algorithms benchmark.</li>
</ul>

<div id="publications">
<h3>Selected Publications</h3>
</div>

<h4><p id="2025-papers">2025</p></h4>
<ul style='margin-top:-0.1in'>
    <li>[OSDI'25] WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training</li>
    <li>[ISCA'25] DCPerf: An Open-Source, Battle-Tested Performance Benchmark Suite for Datacenter Workloads</li>
    <li>[ISCA'25] Meta's Second Generation AI Chip: Model-Chip Co-Design and Productionization Experiences</li>
    <li>[ISCA'25] Scaling Llama 3 Training with Efficient Parallelism Strategies</li>
    <li>[HPCA'25] <a href="https://pasalabs.org/papers/2025/HPCA25_DLRM.pdf">Machine Learning-Guided Memory Optimization for DLRM Inference on Tiered Memory</a></li>
    <li>[ARITH'25] An Empirical Study of Microscaling Formats for Low-Precision LLM Training</li>
    <li>[MLSys'25] <a href="https://arxiv.org/abs/2411.01783">Context Parallelism for Scalable Million-Token Inference</a></li>
    <li>[Communications of the ACM] <a href="https://tangchq74.github.io/Meta-infra.pdf">Meta's Hyperscale Infrastructure: Overview and Insights</a>
</ul>


<h4>2024</h4>
<ul style='margin-top:-0.1in'>
    <li><a href="https://arxiv.org/pdf/2407.21783">The Llama 3 Herd of Models</a></li>

    <li>[<b>IEEE Micro Top Picks'24</b>] <a href="papers/End-to-End_Cloud_Application_Cloning_With_Ditto.pdf"> End-to-End Cloud Application Cloning With Ditto</a></li>

    <li>[<b>IEEE Micro Top Picks'24</b>] <a href="https://tangchq74.github.io/Contiguitas-Top-Picks.pdf">Contiguitas: The Pursuit of Physical Memory Contiguity in Datacenters</a></li>

    <li>[<b>SOSP'24 Best Paper</b>] <a href="https://dl.acm.org/doi/pdf/10.1145/3694715.3695977">FBDetect: Catching Tiny Performance Regressions at Hyperscale through In-Production Monitoring</a></li>

    <li>[<b>OSDI'24 Best Paper</b>] <a href="https://www.usenix.org/system/files/osdi24-chow.pdf">ServiceLab: Preventing Tiny Performance Regressions at Hyperscale through Pre-Production Testing</a></li>

    <li>[OSDI'24] <a href="https://www.usenix.org/system/files/osdi24-choudhury.pdf"> MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale</a></li>

    <li>[OSDI'24] <a href="https://www.usenix.org/system/files/osdi24-kumar.pdf">Optimizing Resource Allocation in Hyperscale Datacenters: Scalability, Usability, and Experiences</a>

    <li>[SC'24] <a href="https://arxiv.org/pdf/2407.04272">Accelerating Communication in Deep Learning Recommendation Model Training with Dual-Level Adaptive Lossy Compression</a></li>

    <li>[ASPLOS'24] <a href="https://dl.acm.org/doi/10.1145/3617232.3624853"> Expanding Datacenter Capacity with DVFS Boosting: A safe and scalable deployment experience</a></li>

    <li>[ICML'24] <a href="https://arxiv.org/pdf/2403.02545">Wukong: Towards a Scaling Law for Large-Scale Recommendation</a></li>

    <li>[MLSys'24] <a href="https://proceedings.mlsys.org/paper_files/paper/2024/file/78834433edc3291f4c6cbbd2759324db-Paper-Conference.pdf">Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large Scale Recommendation</a></li>

    <li>[WWW'24] <a href="https://dl.acm.org/doi/abs/10.1145/3589335.3648304">Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale</a></li>

    <li>[NeurIPS'24] <a href="https://arxiv.org/pdf/2405.14377">CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization</a></li>

    <li>[NeurIPS'24] <a href="https://arxiv.org/abs/2406.11775">Task Me Anything</a></li>

    <li>[NSDI'24] <a href="https://www.usenix.org/system/files/nsdi24-matam.pdf"> QuickUpdate: a Real-Time Personalization System for Large-Scale Recommendation Model</a></li>

    <li>[RecSys'24] <a href="https://dl.acm.org/doi/10.1145/3640457.3688037">Toward 100TB Recommendation Models With Embedding Offloading</a></li>

    <li><a href="https://ai.meta.com/static-resource/movie-gen-research-paper"> Movie Gen: A Cast of Media Foundation Models</a></li>

    <li><a href="https://engineering.fb.com/2024/08/05/data-center-engineering/dcperf-open-source-benchmark-suite-for-hyperscale-compute-applications/"> DCPerf: An open source benchmark suite for hyperscale compute applications </a></li>

    <li><a href="https://www.linkedin.com/posts/hjmshi_announcing-the-results-of-the-inaugural-algoperf-activity-7224879495734870017-rVHn/">PyTorch Distributed Shampoo Winning the MLCommon Training Algorithms Competition</a></li>

</ul>

<h4>2023</h4>

<ul style='margin-top:-0.1in'>
    <li><a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>

    <li>[<b>ISCA'23 Best Paper</b>] <a href="https://dl.acm.org/doi/pdf/10.1145/3579371.3589079"> Contiguitas: The Pursuit of Physical Memory Contiguity in Datacenters</a></li>

    <li>[<b>IEEE Micro Top Picks'23</b>] <a href="https://tangchq74.github.io/IOCost_IEEE_TopPicks_23.pdf">IOCost: Block IO Control for Containers in Datacenters</a>


    <li>[ISCA'23] <a href="http://firoozshahian.com/publications/3579371.3589348.pdf">MTIA: First Generation Silicon Targeting Meta's Recommendation Systems</a></li>

    <li>[ISCA'23] <a href="https://arxiv.org/abs/2302.08007">Shared Microexponents: A Little Shifting Goes a Long Way</a></li>

    <li>[ISCA'23] <a href="https://arxiv.org/pdf/2206.02878">TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory</a></li>

    <li>[ISCA'23] <a href="https://dl.acm.org/doi/10.1145/3579371.3589072"> Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks</a></li>

    <li>[MLSys'23] <a href="https://arxiv.org/abs/2211.05239">RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure</a></li>

    <li>[OSDI'23] <a href="https://www.usenix.org/conference/osdi23/presentation/lai"> AdaEmbed: Adaptive Embedding for Large-Scale Recommendation Models</a></li>

    <li>[ISPASS'23] <a href="https://ieeexplore.ieee.org/document/10158161"> Characterization of Data Compression in Datacenters, </a></li>

    <li>[ASPLOS'23] <a href="https://dl.acm.org/doi/10.1145/3575693.3575751"> Ditto: End-to-End Application Cloning for Networked Cloud Services,</a> </li>

    <li>[VLDB'23] <a href="https://arxiv.org/pdf/2304.11277">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</a></li>

    <li><a href="https://arxiv.org/pdf/2310.10537">Microscaling Data Formats for Deep Learning</a></li>

    <li><a href="https://arxiv.org/abs/2301.02959">FlexShard: Flexible Sharding for Industry-Scale Sequence Recommendation Models</a></li>
</ul>

<h4>2022</h4>
<ul style='margin-top:-0.1in'>
    <li>[<b>ASPLOS'22 Best Paper</b>] <a href="https://tangchq74.github.io/TMO-ASPLOS22.pdf"> TMO: Transparent Memory Offloading in Datacenters</a></li>

    <li>[ASPLOS'22] <a href="https://arxiv.org/abs/2201.10095">RecShard: Statistical Feature-Based Memory Optimization for Industry-Scale Neural Recommendation</a></li>

    <li>[OSDI'22] <a href="https://www.usenix.org/system/files/osdi22-unger.pdf">Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization</a></li>

    <li>[ISCA'22] <a href="https://arxiv.org/pdf/2104.05158">Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models</a></li>

    <li>[NSDI'22] <a href="https://www.usenix.org/system/files/nsdi22-paper-eisenman.pdf"> Check-N-Run: a Checkpointing System for Training Deep Learning Recommendation Models</a></li>

    <li>[ICDCS'22] <a href="https://ieeexplore.ieee.org/abstract/document/9912130"> Supporting Massive DLRM Inference through Software Defined Memory</a></li>

    <li>[DLP-KDD'22] <a href="https://arxiv.org/pdf/2203.11014">DHEN: A deep and hierarchical ensemble network for large-scale click-through rate prediction</a></li>


</ul>

<h4>2021</h4>
<ul style='margin-top:-0.1in'>

    <li>[<b>IEEE Micro Top Picks'21</b>] <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=9390361"> Understanding Acceleration Opportunities at Hyperscale</a></li>

    <li>[Micro'21] <a href="https://ieeexplore.ieee.org/abstract/document/9435938">
Low-Precision Hardware Architectures Meet Recommendation
Model Inference at Scale</a></li>

    <li>[Big Data'21] <a href="https://arxiv.org/abs/2103.00130">Efficient
Soft-Error Detection for Low-precision Deep Learning Recommendation Models</a></li>

    <li>[ATC'21] <a href="https://www.usenix.org/conference/atc21/presentation/kassa">
Improving Performance of Flash Based Key-Value Stores
Using Storage Class Memory as a Volatile Memory Extension</a></li>

    <li><a href="https://arxiv.org/pdf/2107.04140.pdf">First-Generation Inference Accelerator Deployment at Facebook</a></li>

    <li><a href="https://arxiv.org/pdf/2101.05615">FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference</a></li>


</ul>

<h4>2020</h4>
<ul style='margin-top:-0.1in'>
    <li>[ASPLOS'20] <a href="https://research.fb.com/publications/accelerometer-understanding-acceleration-opportunities-for-data-center-overheads-at-hyperscale/"> Accelerometer: Understanding Acceleration Opportunities for Data Center Overheads at Hyperscale</a></li>

    <li>[DLP-KDD'20] <a href="https://research.fb.com/wp-content/uploads/2020/08/Training-Deep-Learning-Recommendation-Model-with-Quantized-Collective-Communications-v2.pdf">Training Deep Learning Recommendation Model with Quantized Collective Communications</a></li>

    <li><a href="https://arxiv.org/abs/2010.11305">Mixed-Precision Embedding Using a Cache</a></li>

    <li><a href="https://engineering.fb.com/2020/11/18/open-source/fiosynth/">FioSynth: A representative I/O benchmark and data visualizer for data center workloads,</a></li>

    <li><a href="https://engineering.fb.com/2020/05/11/data-center-engineering/accelerometer-and-softsku/"> Accelerometer and SoftSKU: Improving hardware platform
performance for diverse microservices</a></li>

</ul>

<h4>2019</h4>
<ul style='margin-top:-0.1in'>
    <li><a href="https://arxiv.org/pdf/1906.00091">Deep Learning Recommendation Model for Personalization and Recommendation Systems</a></li>
    <li>[ISCA'19] <a href="https://research.fb.com/wp-content/uploads/2019/05/SoftSKU-Optimizing-Server-Architectures-for-Microservice-Diversity-@Scale.pdf">SoftSKU: Optimizing Server Architectures for Microservice</a></li>

    <li>[MLSys'19] <a href="https://arxiv.org/abs/1911.02079">Post-Training 4-bit Quantization of Embedding Tables</a></li>

    <li>[MLSys'19] <a href="https://proceedings.mlsys.org/paper_files/paper/2019/file/d59a1dc497cf2773637256f50f492723-Paper.pdf"> Bandana: Using Non-Volatile Memory for Storing Deep Learning Models</a></li>

    <li>[ATC'19] <a href="https://research.fb.com/publications/whos-afraid-of-uncorrectable-bit-errors-online-recovery-of-flash-errors-with-distributed-redundancy/"> Who's Afraid of Uncorrectable Bit Errors? Online Recovery of Flash Errors with Distributed Redundancy</a></li>

    <li><a href="https://arxiv.org/abs/1905.12322">A Study of BFLOAT16 for Deep Learning Training</a></li>

</ul>

</body>

</html>
