<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_c3m3d4lhbmd7-0{list-style-type:none}ul.lst-kix_tqav6y84cgtf-7{list-style-type:none}ul.lst-kix_tqav6y84cgtf-6{list-style-type:none}ul.lst-kix_tqav6y84cgtf-8{list-style-type:none}ul.lst-kix_tvepeaw59b9g-2{list-style-type:none}ul.lst-kix_tqav6y84cgtf-3{list-style-type:none}ul.lst-kix_tvepeaw59b9g-1{list-style-type:none}ul.lst-kix_tqav6y84cgtf-2{list-style-type:none}ul.lst-kix_tvepeaw59b9g-0{list-style-type:none}ul.lst-kix_tqav6y84cgtf-5{list-style-type:none}.lst-kix_c3m3d4lhbmd7-7>li:before{content:"\0025cb   "}ul.lst-kix_tqav6y84cgtf-4{list-style-type:none}ul.lst-kix_tqav6y84cgtf-1{list-style-type:none}.lst-kix_c3m3d4lhbmd7-8>li:before{content:"\0025a0   "}ul.lst-kix_tqav6y84cgtf-0{list-style-type:none}.lst-kix_c3m3d4lhbmd7-2>li:before{content:"\0025a0   "}.lst-kix_c3m3d4lhbmd7-4>li:before{content:"\0025cb   "}.lst-kix_c3m3d4lhbmd7-3>li:before{content:"\0025cf   "}ul.lst-kix_tvepeaw59b9g-6{list-style-type:none}ul.lst-kix_tvepeaw59b9g-5{list-style-type:none}ul.lst-kix_tvepeaw59b9g-4{list-style-type:none}ul.lst-kix_tvepeaw59b9g-3{list-style-type:none}.lst-kix_c3m3d4lhbmd7-6>li:before{content:"\0025cf   "}ul.lst-kix_tvepeaw59b9g-8{list-style-type:none}ul.lst-kix_tvepeaw59b9g-7{list-style-type:none}.lst-kix_c3m3d4lhbmd7-5>li:before{content:"\0025a0   "}ul.lst-kix_c3m3d4lhbmd7-2{list-style-type:none}ul.lst-kix_c3m3d4lhbmd7-1{list-style-type:none}.lst-kix_c3m3d4lhbmd7-0>li:before{content:"\0025cf   "}ul.lst-kix_c3m3d4lhbmd7-4{list-style-type:none}ul.lst-kix_c3m3d4lhbmd7-3{list-style-type:none}.lst-kix_c3m3d4lhbmd7-1>li:before{content:"\0025cb   "}ul.lst-kix_c3m3d4lhbmd7-6{list-style-type:none}ul.lst-kix_c3m3d4lhbmd7-5{list-style-type:none}ul.lst-kix_c3m3d4lhbmd7-8{list-style-type:none}ul.lst-kix_c3m3d4lhbmd7-7{list-style-type:none}.lst-kix_mqbepa7onuvy-4>li:before{content:"\0025cb   "}.lst-kix_mqbepa7onuvy-6>li:before{content:"\0025cf   "}.lst-kix_mqbepa7onuvy-3>li:before{content:"\0025cf   "}.lst-kix_mqbepa7onuvy-7>li:before{content:"\0025cb   "}.lst-kix_mqbepa7onuvy-0>li:before{content:"\0025cf   "}.lst-kix_mqbepa7onuvy-2>li:before{content:"\0025a0   "}.lst-kix_mqbepa7onuvy-8>li:before{content:"\0025a0   "}.lst-kix_ag06ygptcqj8-5>li:before{content:"\0025a0   "}.lst-kix_mqbepa7onuvy-1>li:before{content:"\0025cb   "}.lst-kix_5h1v5tj214r7-8>li:before{content:"\0025a0   "}.lst-kix_ag06ygptcqj8-6>li:before{content:"\0025cf   "}.lst-kix_ag06ygptcqj8-7>li:before{content:"\0025cb   "}.lst-kix_5h1v5tj214r7-6>li:before{content:"\0025cf   "}.lst-kix_5h1v5tj214r7-7>li:before{content:"\0025cb   "}.lst-kix_ag06ygptcqj8-8>li:before{content:"\0025a0   "}.lst-kix_5h1v5tj214r7-5>li:before{content:"\0025a0   "}.lst-kix_tqav6y84cgtf-8>li:before{content:"\0025a0   "}.lst-kix_mqbepa7onuvy-5>li:before{content:"\0025a0   "}.lst-kix_5h1v5tj214r7-2>li:before{content:"\0025a0   "}.lst-kix_5h1v5tj214r7-3>li:before{content:"\0025cf   "}.lst-kix_m0x3ng30pxgy-1>li:before{content:"\0025cb   "}.lst-kix_tqav6y84cgtf-7>li:before{content:"\0025cb   "}.lst-kix_m0x3ng30pxgy-0>li:before{content:"\0025cf   "}.lst-kix_5h1v5tj214r7-0>li:before{content:"\0025cf   "}.lst-kix_5h1v5tj214r7-4>li:before{content:"\0025cb   "}.lst-kix_tqav6y84cgtf-6>li:before{content:"\0025cf   "}.lst-kix_tqav6y84cgtf-3>li:before{content:"\0025cf   "}.lst-kix_tqav6y84cgtf-5>li:before{content:"\0025a0   "}.lst-kix_5h1v5tj214r7-1>li:before{content:"\0025cb   "}.lst-kix_98axzw46idia-8>li:before{content:"\0025a0   "}.lst-kix_tqav6y84cgtf-4>li:before{content:"\0025cb   "}.lst-kix_ag06ygptcqj8-4>li:before{content:"\0025cb   "}.lst-kix_tqav6y84cgtf-1>li:before{content:"\0025cb   "}.lst-kix_ag06ygptcqj8-2>li:before{content:"\0025a0   "}.lst-kix_ag06ygptcqj8-3>li:before{content:"\0025cf   "}.lst-kix_tqav6y84cgtf-2>li:before{content:"\0025a0   "}.lst-kix_ag06ygptcqj8-0>li:before{content:"\0025cf   "}.lst-kix_ag06ygptcqj8-1>li:before{content:"\0025cb   "}.lst-kix_tqav6y84cgtf-0>li:before{content:"\0025cf   "}ul.lst-kix_5h1v5tj214r7-4{list-style-type:none}ul.lst-kix_5h1v5tj214r7-5{list-style-type:none}ul.lst-kix_5h1v5tj214r7-2{list-style-type:none}.lst-kix_vco7omwref9g-5>li:before{content:"\0025a0   "}.lst-kix_vco7omwref9g-7>li:before{content:"\0025cb   "}ul.lst-kix_5h1v5tj214r7-3{list-style-type:none}ul.lst-kix_5h1v5tj214r7-8{list-style-type:none}ul.lst-kix_5h1v5tj214r7-6{list-style-type:none}.lst-kix_vco7omwref9g-2>li:before{content:"\0025a0   "}.lst-kix_vco7omwref9g-6>li:before{content:"\0025cf   "}ul.lst-kix_5h1v5tj214r7-7{list-style-type:none}.lst-kix_vco7omwref9g-1>li:before{content:"\0025cb   "}ul.lst-kix_5h1v5tj214r7-0{list-style-type:none}ul.lst-kix_5h1v5tj214r7-1{list-style-type:none}.lst-kix_vco7omwref9g-0>li:before{content:"\0025cf   "}.lst-kix_vco7omwref9g-8>li:before{content:"\0025a0   "}.lst-kix_98axzw46idia-7>li:before{content:"\0025cb   "}.lst-kix_98axzw46idia-6>li:before{content:"\0025cf   "}.lst-kix_98axzw46idia-5>li:before{content:"\0025a0   "}.lst-kix_vco7omwref9g-3>li:before{content:"\0025cf   "}ul.lst-kix_ag06ygptcqj8-7{list-style-type:none}.lst-kix_98axzw46idia-4>li:before{content:"\0025cb   "}ul.lst-kix_ag06ygptcqj8-8{list-style-type:none}.lst-kix_vco7omwref9g-4>li:before{content:"\0025cb   "}.lst-kix_m0x3ng30pxgy-2>li:before{content:"\0025a0   "}ul.lst-kix_ag06ygptcqj8-3{list-style-type:none}.lst-kix_98axzw46idia-1>li:before{content:"\0025cb   "}.lst-kix_98axzw46idia-2>li:before{content:"\0025a0   "}ul.lst-kix_ag06ygptcqj8-4{list-style-type:none}.lst-kix_m0x3ng30pxgy-3>li:before{content:"\0025cf   "}ul.lst-kix_ag06ygptcqj8-5{list-style-type:none}ul.lst-kix_ag06ygptcqj8-6{list-style-type:none}.lst-kix_m0x3ng30pxgy-4>li:before{content:"\0025cb   "}.lst-kix_98axzw46idia-3>li:before{content:"\0025cf   "}ul.lst-kix_ag06ygptcqj8-0{list-style-type:none}ul.lst-kix_ag06ygptcqj8-1{list-style-type:none}ul.lst-kix_ag06ygptcqj8-2{list-style-type:none}.lst-kix_m0x3ng30pxgy-6>li:before{content:"\0025cf   "}.lst-kix_m0x3ng30pxgy-5>li:before{content:"\0025a0   "}.lst-kix_m0x3ng30pxgy-7>li:before{content:"\0025cb   "}.lst-kix_98axzw46idia-0>li:before{content:"\0025cf   "}.lst-kix_m0x3ng30pxgy-8>li:before{content:"\0025a0   "}ul.lst-kix_mqbepa7onuvy-7{list-style-type:none}ul.lst-kix_mqbepa7onuvy-8{list-style-type:none}ul.lst-kix_m0x3ng30pxgy-7{list-style-type:none}ul.lst-kix_m0x3ng30pxgy-8{list-style-type:none}ul.lst-kix_m0x3ng30pxgy-5{list-style-type:none}ul.lst-kix_m0x3ng30pxgy-6{list-style-type:none}ul.lst-kix_m0x3ng30pxgy-3{list-style-type:none}ul.lst-kix_m0x3ng30pxgy-4{list-style-type:none}ul.lst-kix_vco7omwref9g-3{list-style-type:none}ul.lst-kix_m0x3ng30pxgy-1{list-style-type:none}ul.lst-kix_vco7omwref9g-2{list-style-type:none}ul.lst-kix_m0x3ng30pxgy-2{list-style-type:none}ul.lst-kix_vco7omwref9g-1{list-style-type:none}ul.lst-kix_vco7omwref9g-0{list-style-type:none}ul.lst-kix_m0x3ng30pxgy-0{list-style-type:none}.lst-kix_tvepeaw59b9g-8>li:before{content:"\0025a0   "}ul.lst-kix_98axzw46idia-4{list-style-type:none}.lst-kix_tvepeaw59b9g-3>li:before{content:"\0025cf   "}ul.lst-kix_98axzw46idia-3{list-style-type:none}ul.lst-kix_98axzw46idia-2{list-style-type:none}ul.lst-kix_98axzw46idia-1{list-style-type:none}ul.lst-kix_98axzw46idia-8{list-style-type:none}.lst-kix_tvepeaw59b9g-1>li:before{content:"\0025cb   "}.lst-kix_tvepeaw59b9g-5>li:before{content:"\0025a0   "}ul.lst-kix_98axzw46idia-7{list-style-type:none}ul.lst-kix_98axzw46idia-6{list-style-type:none}.lst-kix_tvepeaw59b9g-0>li:before{content:"\0025cf   "}.lst-kix_tvepeaw59b9g-4>li:before{content:"\0025cb   "}ul.lst-kix_98axzw46idia-5{list-style-type:none}.lst-kix_tvepeaw59b9g-7>li:before{content:"\0025cb   "}ul.lst-kix_vco7omwref9g-8{list-style-type:none}ul.lst-kix_98axzw46idia-0{list-style-type:none}ul.lst-kix_vco7omwref9g-7{list-style-type:none}ul.lst-kix_vco7omwref9g-6{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_vco7omwref9g-5{list-style-type:none}.lst-kix_tvepeaw59b9g-6>li:before{content:"\0025cf   "}ul.lst-kix_vco7omwref9g-4{list-style-type:none}ul.lst-kix_mqbepa7onuvy-1{list-style-type:none}ul.lst-kix_mqbepa7onuvy-2{list-style-type:none}ul.lst-kix_mqbepa7onuvy-0{list-style-type:none}ul.lst-kix_mqbepa7onuvy-5{list-style-type:none}ul.lst-kix_mqbepa7onuvy-6{list-style-type:none}.lst-kix_tvepeaw59b9g-2>li:before{content:"\0025a0   "}ul.lst-kix_mqbepa7onuvy-3{list-style-type:none}ul.lst-kix_mqbepa7onuvy-4{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c2{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c0{background-color:#ffff00;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c5{margin-left:72pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c15{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:23pt;font-family:"Arial";font-style:normal}.c17{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c11{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c16{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c4{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c6{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c1{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c12{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{color:inherit;text-decoration:inherit}.c8{padding:0;margin:0}.c19{background-color:#ffffff;font-size:12pt}.c18{margin-left:108pt;padding-left:0pt}.c10{font-style:italic}.c13{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c12 doc-content"><div><p class="c7 c13"><span class="c4"></span></p></div><p class="c11 title" id="h.xe9aqqzh3cph"><span class="c17">Meta&rsquo;s AI and Systems Co-Design Team</span></p><p class="c7 c13"><span class="c4"></span></p><p class="c7"><span>The AI and Systems Co-Design team at </span><span>Meta (formerly known as Facebook), led by </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://tangchq74.github.io/&amp;sa=D&amp;source=editors&amp;ust=1728602269285227&amp;usg=AOvVaw04XsYnCcgJiclKlhEFhkrI">Chunqiang Tang (a.k.a. CQ Tang)</a></span><span>, consists of over 100 employees, mostly PhDs, including many world-class </span><span class="c1"><a class="c3" href="#id.ntu08f6dqd4i">research scientists and engineers</a></span><span class="c4">. We conduct interdisciplinary research and development across hardware, software, and AI, as reflected in our team name &ldquo;co-design.&rdquo;</span></p><p class="c7 c13"><span class="c4"></span></p><ul class="c8 lst-kix_vco7omwref9g-0 start"><li class="c2 li-bullet-0"><span>We explore, design, and productionize innovative hardware technologies&mdash;from CPUs, GPUs and storage to Meta&rsquo;s custom AI chips&mdash;in Meta&rsquo;s hyperscale fleet of O(1,000,000) servers and O(100,000) GPUs, powering all Meta products, including Facebook, Instagram, and </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=http://meta.ai&amp;sa=D&amp;source=editors&amp;ust=1728602269285741&amp;usg=AOvVaw1Qy_THX7Nb4yBYPOVt4LY0">meta.ai</a></span><span class="c4">.</span></li><li class="c2 li-bullet-0"><span class="c4">We apply novel software optimizations across the whole stack&mdash;from ML models and applications to the Linux kernel&mdash;to achieve optimal performance on the hardware.</span></li><li class="c2 li-bullet-0"><span class="c4">We develop innovative AI technologies for large language models (Llama), ranking, and recommendation.</span></li></ul><p class="c7 c13"><span class="c4"></span></p><p class="c7"><span>Overall, our work largely corresponds to the research communities of hardware architecture (ISCA, ASPLOS), systems for ML (MLSys and ML-related parts of SOSP, OSDI, SIGCOMM, NSDI), ML (NeurIPS, ICML, ICLR) and supercomputing (SC, ICS). Here are selected publications that highlight our work in diverse </span><span>areas</span><span class="c4">.</span></p><p class="c7 c13"><span class="c0"></span></p><ul class="c8 lst-kix_c3m3d4lhbmd7-0 start"><li class="c2 li-bullet-0"><span class="c4">AI chip and server design</span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-1 start"><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=http://firoozshahian.com/publications/3579371.3589348.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269286463&amp;usg=AOvVaw2yuPgMaBHelrB5fmrnzxM0">MTIA: First Generation Silicon Targeting Meta&rsquo;s Recommendation Systems</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://engineering.fb.com/2022/10/18/open-source/ocp-summit-2022-grand-teton/&amp;sa=D&amp;source=editors&amp;ust=1728602269286692&amp;usg=AOvVaw05BA80agEzfrLjFyWj9Z3I">The Grand Teton AI Server</a></span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-0"><li class="c2 li-bullet-0"><span class="c4">Systems for AI</span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-1 start"><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/&amp;sa=D&amp;source=editors&amp;ust=1728602269287024&amp;usg=AOvVaw0hil6kqJ2yW6zNqUuWdsqv">Llama 2: Open foundation and fine-tuned chat models</a></span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-2 start"><li class="c7 c18 li-bullet-0"><span>Our contributions: </span><span>we re-architected Llama&rsquo;s training infrastructure, transitioning from a &nbsp;research environment to Meta&rsquo;s hyperscale production private cloud.</span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-1"><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://ai.meta.com/research/publications/the-llama-3-herd-of-models/&amp;sa=D&amp;source=editors&amp;ust=1728602269287461&amp;usg=AOvVaw3KP6YVHu9G5qsEc8ztjTVo">The Llama 3 Herd of Models</a></span><span class="c4">: </span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-2 start"><li class="c7 c18 li-bullet-0"><span>Our contributions: much of the work described in Section 3.3 </span><span class="c10">&ldquo;Infrastructure, Scaling, and Efficiency&rdquo; , </span><span>Section 6 &ldquo;</span><span class="c10">Inference</span><span>&rdquo;, and Section 7.3</span><span class="c10">&nbsp;&ldquo;Model Scaling.&rdquo;</span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-1"><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2104.05158&amp;sa=D&amp;source=editors&amp;ust=1728602269287877&amp;usg=AOvVaw1lDPSmnm1W3-SregXsRSeX">Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2304.11277&amp;sa=D&amp;source=editors&amp;ust=1728602269288095&amp;usg=AOvVaw1A94OoV369GVkZk4egL4GO">PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel</a></span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-0"><li class="c2 li-bullet-0"><span class="c4">ML model, kernel, numerics, and optimizer</span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-1 start"><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/&amp;sa=D&amp;source=editors&amp;ust=1728602269288399&amp;usg=AOvVaw3cfp_-LiHSt49rXw229VXI">Pruning and Distillation to Enable Llama 3.2 1B and 3B Models Suitable for Mobile Devices</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/1906.00091&amp;sa=D&amp;source=editors&amp;ust=1728602269288624&amp;usg=AOvVaw3LaRbPhBWAdcGMTDTVssQg">Deep Learning Recommendation Model for Personalization and Recommendation Systems</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2403.02545&amp;sa=D&amp;source=editors&amp;ust=1728602269288825&amp;usg=AOvVaw3reHRYZ3UJ8NNZrLZLK_Uk">Wukong: Towards a Scaling Law for Large-Scale Recommendation</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://www.linkedin.com/posts/hjmshi_announcing-the-results-of-the-inaugural-algoperf-activity-7224879495734870017-rVHn/&amp;sa=D&amp;source=editors&amp;ust=1728602269289079&amp;usg=AOvVaw2Bow6gSftCABGfWkKLWFHa">PyTorch Distributed Shampoo Winning the MLCommon Training Algorithms Competition</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://dl.acm.org/doi/abs/10.1145/3589335.3648304&amp;sa=D&amp;source=editors&amp;ust=1728602269289295&amp;usg=AOvVaw07E_uqdJjR8YmoYbe0uNCn">Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2310.10537&amp;sa=D&amp;source=editors&amp;ust=1728602269289493&amp;usg=AOvVaw1iEfeDEmQ4F3_utEbEn63E">Microscaling Data Formats for Deep Learning</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2101.05615&amp;sa=D&amp;source=editors&amp;ust=1728602269289715&amp;usg=AOvVaw0xJDNhIWY96eGOqd0neoIR">FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://pytorch.org/blog/int4-decoding/&amp;sa=D&amp;source=editors&amp;ust=1728602269289920&amp;usg=AOvVaw0KG-o0ZSmT42EFt6ND20WU">INT4 Decoding GQA CUDA Optimizations for LLM Inference</a></span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-0"><li class="c2 li-bullet-0"><span class="c4">HPC and collective communication (MPI, NCCL, RCCL)</span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-1 start"><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2407.04272&amp;sa=D&amp;source=editors&amp;ust=1728602269290217&amp;usg=AOvVaw0tUL2DuvBN8BrNl5Exw-92">Accelerating Communication in Deep Learning Recommendation Model Training with Dual-Level Adaptive Lossy Compression</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://research.fb.com/wp-content/uploads/2020/08/Training-Deep-Learning-Recommendation-Model-with-Quantized-Collective-Communications-v2.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269290518&amp;usg=AOvVaw3IbWyNFHMVMYhVwRPygfAr">Training Deep Learning Recommendation Model with Quantized Collective Communications</a></span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-0"><li class="c2 li-bullet-0"><span class="c4">Performance benchmarking and projection</span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-1 start"><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://engineering.fb.com/2024/08/05/data-center-engineering/dcperf-open-source-benchmark-suite-for-hyperscale-compute-applications/&amp;sa=D&amp;source=editors&amp;ust=1728602269290904&amp;usg=AOvVaw0W_XyclDSIxLqCBLLuXCYH">DCPerf: An open source benchmark suite for hyperscale compute applications</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/&amp;sa=D&amp;source=editors&amp;ust=1728602269291198&amp;usg=AOvVaw0DqgMURO8Fwl5Kb2oiN7gB">DLRM: An advanced, open source deep learning recommendation model</a></span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-0"><li class="c2 li-bullet-0"><span class="c4">Hardware and software co-design</span></li></ul><ul class="c8 lst-kix_c3m3d4lhbmd7-1 start"><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2206.02878&amp;sa=D&amp;source=editors&amp;ust=1728602269291545&amp;usg=AOvVaw025qYDf6PfEYV7RSGTemFT">TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory</a></span></li><li class="c5 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://research.fb.com/wp-content/uploads/2019/05/SoftSKU-Optimizing-Server-Architectures-for-Microservice-Diversity-@Scale.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269291817&amp;usg=AOvVaw1h8B7ivKEmRgH-Hqo0dv2w">SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale</a></span></li></ul><p class="c7 c13"><span class="c4"></span></p><p class="c7"><span>For the areas mentioned above, we are actively hiring managers, PhDs, and experienced engineers, but we are not hiring new graduates with BS or MS degrees. If interested, please submit your resume </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://2024resumedropco-design.splashthat.com/&amp;sa=D&amp;source=editors&amp;ust=1728602269292411&amp;usg=AOvVaw0W4xKITSa2i6mOK5JToUNs">here</a></span><span class="c4">. If you have questions, please email us at codesignATmetaDOTcom.</span></p><p class="c7 c13"><span class="c4"></span></p><p class="c7"><span>While we excel in and strongly encourage research and publication, our primary focus is on developing cutting-edge innovations in hardware, software, and AI, and directly integrating them into production systems that serve billions of people worldwide. This focus on production systems and </span><span class="c10">direct real-world impact </span><span>distinguishes us from traditional research labs that rely on technology transfer for </span><span class="c10">indirect impact</span><span>.</span></p><p class="c7 c13"><span class="c4"></span></p><a id="id.ntu08f6dqd4i"></a><p class="c7"><span>To gain a better understanding of who we are, feel free to explore some profiles of our team </span><span>members: </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://tangchq74.github.io/&amp;sa=D&amp;source=editors&amp;ust=1728602269293206&amp;usg=AOvVaw3djEUihdZPrpzlVsOS1rb_">Chunqiang (CQ) Tang</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3D8xbO51UAAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269293372&amp;usg=AOvVaw2EgvLndvgVVvq6rfWFQiIG">Pavan Balaji</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DHqGW4HIAAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269293535&amp;usg=AOvVaw1uqfUE4J0cO0k9HIsg19qW">Amar Phanishayee</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3Dp5h2zh8AAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269293690&amp;usg=AOvVaw3hjOwcxpOzDauJshwWksz8">Maxim Naumov</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://sites.google.com/site/jongsoopark/home&amp;sa=D&amp;source=editors&amp;ust=1728602269293861&amp;usg=AOvVaw1wpBY2C-wYdkSYw6xawZvW">Jongsoo Park</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3D8rbsmO0AAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269294019&amp;usg=AOvVaw0CLJB9DwDReVbDHQhrhIHh">Abhishek Dhanotia</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DKpnlsFwAAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269294170&amp;usg=AOvVaw2FbZxGUkGa8HZnAfzRJcRD">Joel Coburn</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DyxUD8q4AAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269294320&amp;usg=AOvVaw0ZcYk0uVJNkQDG7L5ayK-k">Doe Hyun Yoon</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3D6PyfjT4AAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269294469&amp;usg=AOvVaw1ykTxlyzN_tQFG3ZZlCn0n">Ehsan K. Ardestani</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DyUXXPwYAAAAJ%26hl%3Den&amp;sa=D&amp;source=editors&amp;ust=1728602269294633&amp;usg=AOvVaw2JAePBxwLFynOrkA7tot0A">Satish Nadathur</a></span><span>, Changkyu Kim, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DCe2uZUYAAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269294788&amp;usg=AOvVaw1Dl8w8VLMwfsEGcd6KgOzJ">Bangsheng Tang</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3DmD2TUGQAAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269294948&amp;usg=AOvVaw19ex5GFs1hvQXLvHR-y-Y8">Mustafa Ozdal</a></span><span>, </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://scholar.google.com/citations?user%3D5h4eSNQAAAAJ%26hl%3Den%26oi%3Dao&amp;sa=D&amp;source=editors&amp;ust=1728602269295100&amp;usg=AOvVaw032jMmxg0sUmrvwtsMS8ng">Jianyu Huang</a></span><span>, and </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://dblp.org/pid/92/5162.html&amp;sa=D&amp;source=editors&amp;ust=1728602269295242&amp;usg=AOvVaw1D43R43mqxTLu3bbf4Pcqk">Wenyin Fu</a></span><span class="c4">.</span></p><h1 class="c16" id="h.r2quqn5ki81e"><span>Open Source Projects</span></h1><p class="c7"><span class="c4">Please add links to open source projects that we contributed to.</span></p><ul class="c8 lst-kix_tqav6y84cgtf-0 start"><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://engineering.fb.com/2024/08/05/data-center-engineering/dcperf-open-source-benchmark-suite-for-hyperscale-compute-applications/&amp;sa=D&amp;source=editors&amp;ust=1728602269295741&amp;usg=AOvVaw3iU59bcY51KfESPQDOQY2k">DCPerf: An open source benchmark suite for hyperscale compute applications</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://ai.meta.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/&amp;sa=D&amp;source=editors&amp;ust=1728602269295996&amp;usg=AOvVaw1fT247VteCtDcWYIq6oM7l">DLRM: An advanced, open source deep learning recommendation model</a></span></li><li class="c2 li-bullet-0"><span>ML kernels: </span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://fburl.com/rcll8ztm&amp;sa=D&amp;source=editors&amp;ust=1728602269296210&amp;usg=AOvVaw2GWcei3Rzo8ItyAoFepHz9">FBGEMM (Facebook GEneral Matrix Multiplication) and FBGEMM_GPU(</a></span><span class="c1 c19"><a class="c3" href="https://www.google.com/url?q=https://fburl.com/rcll8ztm&amp;sa=D&amp;source=editors&amp;ust=1728602269296319&amp;usg=AOvVaw0WYIkQiVagbkk2XJTKzVib">FBGEMM GPU Kernels Library</a></span><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://fburl.com/rcll8ztm&amp;sa=D&amp;source=editors&amp;ust=1728602269296418&amp;usg=AOvVaw2ye2zQA7aFBT3AHhDQtlPe">) </a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://github.com/facebookresearch/optimizers/blob/main/distributed_shampoo/README.md&amp;sa=D&amp;source=editors&amp;ust=1728602269296650&amp;usg=AOvVaw1ZDeJh91TcUTBoakHEneWz">Pytorch distributed Shampoo optimizer</a></span></li></ul><h1 class="c16" id="h.w9asws8yn66t"><span>Selected Publication</span><span class="c14">s</span></h1><h2 class="c6" id="h.7850t3g0e05"><span class="c9">2024</span></h2><ul class="c8 lst-kix_mqbepa7onuvy-0 start"><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2407.21783&amp;sa=D&amp;source=editors&amp;ust=1728602269297093&amp;usg=AOvVaw03KidNgbjXi0xrNGFlNwDm">The Llama 3 Herd of Models</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2407.04272&amp;sa=D&amp;source=editors&amp;ust=1728602269297331&amp;usg=AOvVaw0pFwLTTbAldq-4ilpbJgWa">Accelerating Communication in Deep Learning Recommendation Model Training with Dual-Level Adaptive Lossy Compression</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2403.02545&amp;sa=D&amp;source=editors&amp;ust=1728602269297524&amp;usg=AOvVaw2eoCjOI0p7e-g6LKGKS5Ut">Wukong: Towards a Scaling Law for Large-Scale Recommendation</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://proceedings.mlsys.org/paper_files/paper/2024/file/78834433edc3291f4c6cbbd2759324db-Paper-Conference.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269297779&amp;usg=AOvVaw0eTmVX_s3DC3aa28Ue1aS5">Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large Scale Recommendation</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://www.linkedin.com/posts/hjmshi_announcing-the-results-of-the-inaugural-algoperf-activity-7224879495734870017-rVHn/&amp;sa=D&amp;source=editors&amp;ust=1728602269298066&amp;usg=AOvVaw24ONrpiLSUxXtv9Zy_f3-c">PyTorch Distributed Shampoo Winning the MLCommon Training Algorithms Competition</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://dl.acm.org/doi/abs/10.1145/3589335.3648304&amp;sa=D&amp;source=editors&amp;ust=1728602269298284&amp;usg=AOvVaw0hmqj0lFXuij7-gMQ7rmSx">Rankitect: Ranking Architecture Search Battling World-class Engineers at Meta Scale</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2405.14377&amp;sa=D&amp;source=editors&amp;ust=1728602269298482&amp;usg=AOvVaw04cOs7oXjfGHZPQ1Nu_Tgk">CoMERA: Computing- and Memory-Efficient Training via Rank-Adaptive Tensor Optimization</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://www.usenix.org/system/files/osdi24-choudhury.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269298691&amp;usg=AOvVaw2aSX1RDtOBj_ZhFJRAqXaT">MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://www.usenix.org/system/files/nsdi24-matam.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269298910&amp;usg=AOvVaw03KGr6BzdgIDtc0dJjN0t0">QuickUpdate: a Real-Time Personalization System for Large-Scale Recommendation Model</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3640457.3688037&amp;sa=D&amp;source=editors&amp;ust=1728602269299129&amp;usg=AOvVaw0R-gr2ie1sTuPOPPufhWT9">Toward 100TB Recommendation Models With Embedding Offloading</a></span></li></ul><h2 class="c6" id="h.vfrqaiz5arqr"><span class="c9">2023</span></h2><ul class="c8 lst-kix_c3m3d4lhbmd7-0"><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=http://firoozshahian.com/publications/3579371.3589348.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269299437&amp;usg=AOvVaw2KjctBEHsPVf0Bkf_X0r2I">MTIA: First Generation Silicon Targeting Meta&rsquo;s Recommendation Systems</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2310.10537&amp;sa=D&amp;source=editors&amp;ust=1728602269299663&amp;usg=AOvVaw0rRu2bnWO-L-t0gSl6fFvk">Microscaling Data Formats for Deep Learning</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/abs/2302.08007&amp;sa=D&amp;source=editors&amp;ust=1728602269299858&amp;usg=AOvVaw3to6rloNz3Z0-MHHHqCQt5">Shared Microexponents: A Little Shifting Goes a Long Way</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2206.02878&amp;sa=D&amp;source=editors&amp;ust=1728602269300050&amp;usg=AOvVaw0zRGPNRsCXzH6iV00eA9Uj">TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://dl.acm.org/doi/pdf/10.1145/3579371.3589079&amp;sa=D&amp;source=editors&amp;ust=1728602269300264&amp;usg=AOvVaw0EsffeVnlhMrVAdDv8mWqE">Contiguitas: The Pursuit of Physical Memory Contiguity in Datacenters</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://dl.acm.org/doi/10.1145/3579371.3589072&amp;sa=D&amp;source=editors&amp;ust=1728602269300473&amp;usg=AOvVaw16-blYdcvYUKyMWPZw0wvq">Mystique: Enabling Accurate and Scalable Generation of Production AI Benchmarks</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/abs/2211.05239&amp;sa=D&amp;source=editors&amp;ust=1728602269300661&amp;usg=AOvVaw1ElUXArtbOwzdUp53NqD27">RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://www.usenix.org/conference/osdi23/presentation/lai&amp;sa=D&amp;source=editors&amp;ust=1728602269300866&amp;usg=AOvVaw2q6k4t4SgrI6kNtsydsU6D">AdaEmbed: Adaptive Embedding for Large-Scale Recommendation Models</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/&amp;sa=D&amp;source=editors&amp;ust=1728602269301141&amp;usg=AOvVaw1hSXXEFTnOSSCMdNhlxawZ">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></span></li></ul><p class="c7 c13"><span class="c4"></span></p><h2 class="c6" id="h.ys88uc17eo0a"><span class="c9">2022</span></h2><ul class="c8 lst-kix_5h1v5tj214r7-0 start"><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2104.05158&amp;sa=D&amp;source=editors&amp;ust=1728602269301613&amp;usg=AOvVaw3_VOpqu0jG5v-bL-4yXL-u">Software-Hardware Co-design for Fast and Scalable Training of Deep Learning Recommendation Models</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2203.11014&amp;sa=D&amp;source=editors&amp;ust=1728602269301843&amp;usg=AOvVaw0rwNwXqJNfVUJ6fm1cdN4U">DHEN: A deep and hierarchical ensemble network for large-scale click-through rate prediction</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://tangchq74.github.io/TMO-ASPLOS22.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269302116&amp;usg=AOvVaw1O9MVupvlfpc9ASQz3jZJk">TMO: Transparent Memory Offloading in Datacenters</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://www.lockshaw.io/static/unity.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269302338&amp;usg=AOvVaw1VM7NCtwHsoVmJjEMBsvKS">Unity: A Unified Graph Representation and Runtime for Distributed DNN Training</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://www.usenix.org/system/files/nsdi22-paper-eisenman.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269302551&amp;usg=AOvVaw0SRZAxfBe93wHQF4lfhLj7">Check-N-Run: a Checkpointing System for Training Deep Learning Recommendation Models</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/9912130&amp;sa=D&amp;source=editors&amp;ust=1728602269302761&amp;usg=AOvVaw3uIYFVg3-BOTG9B0SBe7qE">Supporting Massive DLRM Inference through Software Defined Memory</a></span></li></ul><h2 class="c6" id="h.vtnn9fvu6hm7"><span class="c9">2021</span></h2><ul class="c8 lst-kix_c3m3d4lhbmd7-0"><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2101.05615&amp;sa=D&amp;source=editors&amp;ust=1728602269303040&amp;usg=AOvVaw3r7w-uaL108NMm3sgImIBa">FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/9435938&amp;sa=D&amp;source=editors&amp;ust=1728602269303256&amp;usg=AOvVaw0dFFha4ss78czXYAJTU6Ti">Low-Precision Hardware Architectures Meet Recommendation Model Inference at Scale</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/abs/2103.00130&amp;sa=D&amp;source=editors&amp;ust=1728602269303450&amp;usg=AOvVaw0cUgoEy0O1eFzMeCBZ24-3">Efficient Soft-Error Detection for Low-precision Deep Learning Recommendation Models</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/2107.04140.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269303653&amp;usg=AOvVaw0LRkp9mPAoRf2AGXPTGD_o">First-Generation Inference Accelerator Deployment at Facebook</a></span></li></ul><h2 class="c6" id="h.ho9f8gd58l2m"><span class="c9">2020</span></h2><ul class="c8 lst-kix_m0x3ng30pxgy-0 start"><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://research.fb.com/wp-content/uploads/2020/08/Training-Deep-Learning-Recommendation-Model-with-Quantized-Collective-Communications-v2.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269304088&amp;usg=AOvVaw0GLhJ1vPPOXKLoBrGbJXJZ">Training Deep Learning Recommendation Model with Quantized Collective Communications</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/abs/2010.11305&amp;sa=D&amp;source=editors&amp;ust=1728602269304307&amp;usg=AOvVaw2Qch8IC61L-c2olIeuxf6_">Mixed-Precision Embedding Using a Cache</a></span></li></ul><h2 class="c6" id="h.bbfqscezndcs"><span class="c9">2019</span></h2><ul class="c8 lst-kix_c3m3d4lhbmd7-0"><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/pdf/1906.00091&amp;sa=D&amp;source=editors&amp;ust=1728602269304716&amp;usg=AOvVaw2uxELZSCpkYxhdKQaxhgJy">Deep Learning Recommendation Model for Personalization and Recommendation Systems</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://research.fb.com/wp-content/uploads/2019/05/SoftSKU-Optimizing-Server-Architectures-for-Microservice-Diversity-@Scale.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269305002&amp;usg=AOvVaw1S1NIDDnSvBRHKa0RTR8rm">SoftSKU: Optimizing Server Architectures for Microservice Diversity @Scale</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/abs/1905.12322&amp;sa=D&amp;source=editors&amp;ust=1728602269305235&amp;usg=AOvVaw2AbZ2JeIuhD4wgjrP9pg_2">A Study of BFLOAT16 for Deep Learning Training</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://arxiv.org/abs/1911.02079&amp;sa=D&amp;source=editors&amp;ust=1728602269305437&amp;usg=AOvVaw1Lw558jWs_6kDMoxZwrDIw">Post-Training 4-bit Quantization of Embedding Tables</a></span></li><li class="c2 li-bullet-0"><span class="c1"><a class="c3" href="https://www.google.com/url?q=https://proceedings.mlsys.org/paper_files/paper/2019/file/d59a1dc497cf2773637256f50f492723-Paper.pdf&amp;sa=D&amp;source=editors&amp;ust=1728602269305676&amp;usg=AOvVaw05VsCLUYcolwr7nnRwIhk0">Bandana: Using Non-Volatile Memory for Storing Deep Learning Models</a></span></li></ul><div><p class="c7 c13"><span class="c4"></span></p></div></body></html>